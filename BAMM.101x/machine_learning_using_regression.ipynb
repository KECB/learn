{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Machine learning using Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Read the data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generate a few summary statistics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data set 1: Rocks vs. Mines</h3>\n",
    "<li>Independent variables: sonar soundings at different frequencies\n",
    "<li>Dependent variable (target): Rock or Mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "df = pd.read_csv(url,header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>See all columns</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=70\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Examine the distribution of the data in column 4</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Quartile 1: from .0067 to .03805\n",
    "<li>Quartile 2: from .03805 to .0625\n",
    "<li>Quartile 3: from .0625 to .100275\n",
    "<li>Quartile 4: from .100275 to .401\n",
    "\n",
    "<h4>Quartile 4 is much larger than the other quartiles. This raises the possibility of outliers</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> A Quantile - Quantile (qq) plot can help identify outliers</h4>\n",
    "<li>y-axis contains values\n",
    "<li>x-axis is the cumulative normal density function plotted as a straight line (-3 to +3)\n",
    "<li>y-axis is the values ordered from lowest to highest\n",
    "<li>the closer the curve is to the line, the more it reflects a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pylab \n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "   \n",
    "stats.probplot(df[4], dist=\"norm\", plot=pylab)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Examine the dependent variable</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[60].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Examine correlations</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "plot.pcolor(df.corr())\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.corr()[0].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Highly correlated items = not good!</h4>\n",
    "<h4>Low correlated items = good </h4>\n",
    "<h4>Correlations with target (dv) = good (high predictive power)</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Set 2: Wine data</h3>\n",
    "<li>Independent variables: Wine composition (alcohol content, sulphites, acidity, etc.)\n",
    "<li>Dependent variable (target): Taste score (average of a panel of 3 wine tasters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "w_df = pd.read_csv(url,header=0,sep=';')\n",
    "w_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_df['volatile acidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "plot.pcolor(w_df.corr())\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Examining the correlation of one variable with the others</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_df.corr()['fixed acidity'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pandas scatter matrix function helps visualize the relationship between features</h3>\n",
    "Use with care though, because it is processor intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "p=scatter_matrix(w_df, alpha=0.2, figsize=(12, 12), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>And we can examine quintile plots as we did with the rocks and mines data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pylab \n",
    "import scipy.stats as stats\n",
    "%matplotlib inline\n",
    "   \n",
    "stats.probplot(w_df['alcohol'], dist=\"norm\", plot=pylab)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training a classifier on Rocks vs Mines</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pylab as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "df = pd.read_csv(url,header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert labels R and M to 0 and 1</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[60]=np.where(df[60]=='R',0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Divide the dataset into training and test samples</h4>\n",
    "<h4>Separate out the x and y variable frames for the train and test samples</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.3)\n",
    "x_train = train.iloc[0:,0:60]\n",
    "y_train = train[60]\n",
    "x_test = test.iloc[0:,0:60]\n",
    "y_test = test[60]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build the model and fit the training data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "model.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Interpreting categorical prediction results</h1>\n",
    "<h3>Precision\n",
    "<h3>Recall\n",
    "<h3>True Positive Rate\n",
    "<h3>False Positive Rate\n",
    "<h3>Precision recall curve\n",
    "<h3>ROC curve\n",
    "<h3>F-Score\n",
    "<h3>Area under PR curve\n",
    "<h3>Area under ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generate predictions in-sample error</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_predictions = model.predict(x_train)\n",
    "print(np.mean((training_predictions - y_train) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Train R-Square:',model.score(x_train,y_train))\n",
    "print('Test R-Square:',model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>These are horrible!</h3>\n",
    "<b>But do we really care?</b>\n",
    "<li>Focus on the problem\n",
    "<li>Do we need to recognize both rocks as well as mines correctly?\n",
    "<li>How do we interpret the predicted y-values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(max(training_predictions),min(training_predictions),np.mean(training_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>We want to predict categories: Rocks or Mines</h2>\n",
    "<h2>But we're actually getting a continuous value</h2>\n",
    "<h2>Not the same thing. So R-Square probably doesn't mean a whole lot</h2>\n",
    "\n",
    "<h2>We need to convert the conitnuous values into  categorical 1s and 0s. We can do this by fixing a threshold value between 0 and 1</h2>\n",
    "<h2>Values greater than the threshold are 1 (Mines). Values less than or equal to the threshold are 0 (Rocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion matrix</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Reports the proportion of\n",
    "<ol>\n",
    "<li><b>true positive</b>: predicts mine and is a mine\n",
    "<li><b>false positive</b>: predicts mine and is not a mine\n",
    "<li><b>true negative</b>: predicts not mine and is not a mine\n",
    "<li><b>false negative</b>:Predicts not mine but turns out to be a mine (BOOM!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(predicted, actual, threshold):\n",
    "    if len(predicted) != len(actual): return -1\n",
    "    tp = 0.0\n",
    "    fp = 0.0\n",
    "    tn = 0.0\n",
    "    fn = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] > 0.5: #labels that are 1.0  (positive examples)\n",
    "            if predicted[i] > threshold:\n",
    "                tp += 1.0 #correctly predicted positive\n",
    "            else:\n",
    "                fn += 1.0 #incorrectly predicted negative\n",
    "        else:              #labels that are 0.0 (negative examples)\n",
    "            if predicted[i] < threshold:\n",
    "                tn += 1.0 #correctly predicted negative\n",
    "            else:\n",
    "                fp += 1.0 #incorrectly predicted positive\n",
    "    rtn = [tp, fn, fp, tn]\n",
    "\n",
    "    return rtn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_predictions = model.predict(x_test)\n",
    "confusion_matrix(testing_predictions,np.array(y_test),0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Misclassification rate = (fp + fn)/number of cases</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(testing_predictions,np.array(y_test),0.5)\n",
    "misclassification_rate = (cm[1] + cm[2])/len(y_test)\n",
    "misclassification_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Precision and Recall</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[tp, fn, fp, tn] = confusion_matrix(testing_predictions,np.array(y_test),0.5)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f_score = 2 * (precision * recall)/(precision + recall)\n",
    "print(precision,recall,f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion matrix (and hence precision, recall etc.) depend on the selected threshold</h2>\n",
    "<h4>As the threshold changes, we will need to tradeoff precision and recall</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[tp, fn, fp, tn] = confusion_matrix(testing_predictions,np.array(y_test),0.9)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f_score = 2 * (precision * recall)/(precision + recall)\n",
    "print(precision,recall,f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ROC: Receiver Order Characteristic</h2>\n",
    "<li>An ROC curve shows the performance of a binary classifier as the threshold varies.\n",
    "<li>Computes two series:\n",
    "<ol>\n",
    "<li>False positive rate (FPR) Fall out/false alarm  = False Positives/(True Negatives + False Positives)\n",
    "<ul><li>Or, what proportion of rocks  are identified as mines</ul>\n",
    "\n",
    "<li>True Positive rate (TPR) Sensitivity/recall = True Positives/(True Positives + False Negatives)\n",
    "<ul><li>Or, what proportion of actual mines are identified as mines</ul>\n",
    "</ol>\n",
    "<ul>\n",
    "<li><b>true positive</b>: predicts mine and is a mine\n",
    "<li><b>false positive</b>: predicts mine and is not a mine\n",
    "<li><b>true negative</b>: predicts not mine and is not a mine\n",
    "<li><b>false negative</b>:Predicts not mine but turns out to be a mine (BOOM!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Let's first plot the predictions against actuals<h3>\n",
    "The goal is to see if our classifier has discriminated at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positives = list()\n",
    "negatives = list()\n",
    "actual = np.array(y_train)\n",
    "for i in range(len(y_train)):\n",
    "    \n",
    "    if actual[i]:\n",
    "        positives.append(training_predictions[i])\n",
    "    else:\n",
    "        negatives.append(training_predictions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_p = pd.DataFrame(positives)\n",
    "df_n = pd.DataFrame(negatives)\n",
    "fig, ax = plt.subplots()\n",
    "a_heights, a_bins = np.histogram(df_p)\n",
    "b_heights, b_bins = np.histogram(df_n, bins=a_bins)\n",
    "width = (a_bins[1] - a_bins[0])/3\n",
    "ax.bar(a_bins[:-1], a_heights, width=width, facecolor='cornflowerblue')\n",
    "ax.bar(b_bins[:-1]+width, b_heights, width=width, facecolor='seagreen')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Repeat for the holdout sample</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positives = list()\n",
    "negatives = list()\n",
    "actual = np.array(y_test)\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    if actual[i]:\n",
    "        positives.append(testing_predictions[i])\n",
    "    else:\n",
    "        negatives.append(testing_predictions[i])\n",
    "df_p = pd.DataFrame(positives)\n",
    "df_n = pd.DataFrame(negatives)\n",
    "fig, ax = plt.subplots()\n",
    "a_heights, a_bins = np.histogram(df_p)\n",
    "b_heights, b_bins = np.histogram(df_n, bins=a_bins)\n",
    "width = (a_bins[1] - a_bins[0])/3\n",
    "ax.bar(a_bins[:-1], a_heights, width=width, facecolor='cornflowerblue')\n",
    "ax.bar(b_bins[:-1]+width, b_heights, width=width, facecolor='seagreen')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Drawing the ROC Curve</h2>\n",
    "<h3>sklearn has a function roc_curve that does this for us</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>In-sample ROC Curve</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(fpr, tpr, thresholds) = roc_curve(y_train,training_predictions)\n",
    "area = auc(fpr,tpr)\n",
    "pl.clf() #Clear the current figure\n",
    "pl.plot(fpr,tpr,label=\"In-Sample ROC Curve with area = %1.2f\"%area)\n",
    "\n",
    "pl.plot([0, 1], [0, 1], 'k') #This plots the random (equal probability line)\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('In sample ROC rocks versus mines')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Out-sample ROC curve</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(fpr, tpr, thresholds) = roc_curve(y_test,testing_predictions)\n",
    "area = auc(fpr,tpr)\n",
    "pl.clf() #Clear the current figure\n",
    "pl.plot(fpr,tpr,label=\"Out-Sample ROC Curve with area = %1.2f\"%area)\n",
    "\n",
    "pl.plot([0, 1], [0, 1], 'k')\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('Out sample ROC rocks versus mines')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(fpr, tpr, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>So, what threshold should we actually use?</h2>\n",
    "<h4>ROC curves and AUC give you a sense for how good your classifier is and how sensitive it is to changes in threshold</h4>\n",
    "<h4>Too sensitive is not good</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example: Let's say</h3>\n",
    "<li>Everything classified as a rock needs to be checked with a hand scanner at 200/scan</li> \n",
    "<li>Everything classified as a mine needs to be defused at 1000 if it is a real mine or 300 if it turns out to be a rock</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(testing_predictions,np.array(y_test),.1)\n",
    "cost1 = 1000*cm[0] + 300 * cm[2] + 200 * cm[1] + 200 * cm[3]\n",
    "cm = confusion_matrix(testing_predictions,np.array(y_test),.9)\n",
    "cost2 = 1000*cm[0] + 300 * cm[2] + 200 * cm[1] + 200 * cm[3]\n",
    "\n",
    "print(cost1,cost2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example: Let's say</h3>\n",
    "<li>Everything classified as a rock will be assumed a rock and if wrong, will cost 5000 in injuries</li> \n",
    "<li>Everything classified as a mine will be left as is (no one will walk on it!)</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(testing_predictions,np.array(y_test),.1)\n",
    "cost1 = 0*cm[0] + 0 * cm[2] + 5000 * cm[1] + 0 * cm[3]\n",
    "cm = confusion_matrix(testing_predictions,np.array(y_test),.9)\n",
    "cost2 = 0*cm[0] + 0 * cm[2] + 5000 * cm[1] + 0 * cm[3]\n",
    "print(cost1,cost2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bottom line. Depends on factors from your domain</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
